% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ds.train_boosted_tree.R
\name{ds.train_boosted_tree}
\alias{ds.train_boosted_tree}
\title{Training of a Gradient Boosted Decision Tree}
\usage{
ds.train_boosted_tree(
  data_name,
  bounds_and_levels,
  output_var,
  drop_columns = NULL,
  drop_NA = TRUE,
  train_test_ratio = 0.9,
  federation,
  max_treecount = 10L,
  max_splits = 5L,
  split_method,
  loss_function,
  amt_spp,
  feature_subsampling = NULL,
  cand_select = c(numeric = "ithess", factor = "exact"),
  weight_update,
  reg_par = c(lambda = 5, gamma = 5),
  shrinkage = 0.1,
  dropout_rate = 0.05,
  ithess_stop = max_treecount,
  seed = NULL,
  datasources = NULL
)
}
\arguments{
\item{data_name}{The name under which the data is saved on the server.}

\item{bounds_and_levels}{Bounds for numeric columns and levels for factors.}

\item{output_var}{The name of the column containing the output variable.}

\item{drop_columns}{Vector of data columns which shall be removed.}

\item{drop_NA}{If NA data in the output variable should be removed.}

\item{train_test_ratio}{Percentage of the data which should be used for
Training.}

\item{federation}{Through which method we share the data between client and
the servers.}

\item{max_treecount}{Maximum amount of trees to build our boosted decision
tree.}

\item{max_splits}{The maximum amount of splits in the trained tree.}

\item{split_method}{Through which method we choose the tree-splits.}

\item{loss_function}{The name of the loss function we want to use for our
boosted tree.}

\item{amt_spp}{The amount of split-points per feature.}

\item{feature_subsampling}{Which part of the feature space we use to build
the trees.}

\item{cand_select}{Splitting-point selection for numeric and factor features.}

\item{weight_update}{Through which method we choose the weights for our tree.}

\item{reg_par}{Regularisation parameter which prevent overfitting.}

\item{shrinkage}{How high the newly trained tree effects the boosted tree.}

\item{dropout_rate}{Chance that a tree is not used for building the next
tree.}

\item{ithess_stop}{Maximum amount of times we update the split-point
candidates if the split-method is "totally_random"}

\item{seed}{If we want to choose a specific random behavior client side.}

\item{datasources}{DATASHIELD server connection.}
}
\value{
The trained decision tree model.
}
\description{
Training of a Gradient Boosted Decision Tree
}
